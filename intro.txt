\chapter{Introduction}
% Basically just put the thesis proposal here

\section{Context}
% \subsection{What is a Partitioning Scheme?}
% \siva{I am not sure what you mean by cloud computing here. Talk about databases.}
Much of the modern world relies on high-performance databases to function.  In order to perform well, these databases must be structured in such a way that queries can read data quickly.\par 
There are generally two factors that contribute most to the amount of time queries take.  The first of these is the number of tuples that the query needs to scan in order to find relevant tuples.  Databases often contain billions of tuples, but queries generally only want a small subset of that.
%, so they need to be very clever about how they store their data in order to operate efficiently. \siva{I do not follow what you mean by random accesses being slow communication between machines taking a long time. random accesses being slow is a property of the hardware.} Many of the queries \st{these computers receive} require scanning large chunks of data, which means that traditional indexes such as B+ trees, which require a lot of random accesses, are ineffective. \siva{B+ trees need random accesses when used as a secondary index not when used as a primary index for partitioning.} 
If every query had to scan every single tuple in the database to find the relevant ones, they would all be incredibly slow.  As such, databases rely on \textit{partitioning}, which is to say, breaking the data down into subsets called \textit{blocks}.  These blocks are generally stored using file formats such as Parquet \cite{parquet} or ORC \cite{orc}.  These formats divide the tuples further into subsets called \textit{row groups} (they are called stripes in ORC, but we will refer to them as row groups throughout this thesis).  With databases divided into blocks, and blocks further divided into row groups, queries can skip the blocks and row groups that they know will not contain any relevant information.% \siva{what is a cloud datastructure?}  When data is partitioned, queries that are only looking for a certain portions of the data can skip over the blocks that do not contain any relevant data.
\par 
The second factor that contributes most to query time is I/O. Performing an I/O to access individual an individual block takes significantly more time than sequentially reading tuples, but is also necessary for skipping blocks.  This introduces a tradeoff between decreasing the number of tuples in each block so that queries can skip with finer granularity and read less tuples overall, and decreasing the number of blocks that queries have to access to minimize I/O cost.
% \siva{In the first paragraph, set up the premise. Do not go into details about why B+ trees or anything is ineffective. The goal of partitioning is not to just reduce the number of random accesses. It is to reduce the total amount of the data that is scanned. Random access is a disk-specific term. I an not sure what that would mean in a cloud context. Say that scan with filters is important. Reducing the amount of the data that is scanned is important for performance. so dbmss partition that data into blocks and collect metadata or index for these blocks. queries use this metadata or index to skip unnecessary data blocks.}
\par Optimizing this tradeoff makes the question of how to best partition data extremely complicated, and there are many different ways of doing so.  These methods of partitioning data are called \textit{partitioning schemes}, and there is plenty of research which has gone into figuring out the best ones for a given dataset and query workload.  A large body of this research has focused on the fact that%\siva{This needs a lot of citations over a variety of different techniques. also say based on dataset and workload.}
% \subsection{An Overview of Modern Partitioning Schemes}
% Perhaps the simplest partitioning scheme is a range partition on one dimension. \siva{what is 'even'? Range partition does not require that all partitions are the same size. It can be on multiple dimensions as well.}  
% In this case, the data is sorted on one dimension and then divided into blocks along that dimension.  This partitioning scheme works well when most queries filter on only one dimension. % \st{and the data structure can easily retrieve data based on its position in that single dimension.} 
% \par 
% However, single-dimension partitioning schemes become a lot less useful when queries filter on multiple dimensions.  And even multi-dimensional sorting algorithms such as z-order \cite{z-order} are not generally used as partitioning schemes - they are usually used within partitions to help speed up queries. \siva{z-order is a very commonly used partition scheme to skip data along multiple dimensions.} 
% \siva{There is plenty of old research on automatically picking range partitioning or z-order based on the workload. Say when these techniques don't work for z-orders even when you know the data and workload.}
% As such, variations on the one-dimensional index have been developed that can take multiple dimensions into account.  Some of these variants include multi-dimensional sorts, where elements are sorted on one dimension, and ties are broken by sorting on another dimension.  One of the most commonly used multi-dimensional sorting algorithms is z-ordering \cite{z-order}, where the bits of different dimensions are interleaved to create a new dimension, which is then sorted on.\par 
it is very difficult to optimize a database without knowing what kinds of queries the database will be seeing. As such, more recent partitioning schemes such as Tsunami \cite{flood}, Qd-Tree \cite{qd}, and Pando \cite{pando} take in a sample of queries called a \textit{workload} to help it optimize its layout. 
% \siva{If you wanna set this us up as two parts where first part is work on selecting parititioning techniques based on data alone and second is on based on both data and workload, then you need to rewrite the previous paragraphto talk about techniques that only use data with citations. Range partitioning, z-order etc are well studied in the context of both data and workload aware partitioning. I will recommend keeping the current flow and not discussing partitioning schemes that are not workload aware.} \st{A partitioning scheme that relies on a workload is said to be \textit{workload-aware}.}
% One of these workload-aware partitioning schemes is Flood \cite{flood}, which divides tuples into cells of a grid on every numerical dimension, and it chooses how many ways to divide each dimension by running gradient descent on the time it takes the workload to query the database.  Tsunami \cite{tsunami} improves on Flood by allowing different divisions of each dimension to have different sizes to account for skew in the workload, and it is also optimized by analyzing workload performance.  Qd-tree \cite{qd} divides datasets in a tree structure where each node of the tree is a predicate dividing the dataset at that node into two pieces, where the predicates are chosen by what would minimize the number of tuples the workload would have to read at that node.  
Qd-tree will be covered significantly in later sections of this paper.\par 
%\siva{You need to say what the novelty of these techniques are in one line.}
\section{Motivation}
\subsection{The Drawbacks of Modern Partitioning Schemes}
% \siva{this subsection is kinda long-winded and hard to follow. How about something like this: (WIP)
% Many real-world workloads have large batch queries and small interactive queries.
% We care about latency, especially the interactive queries.
% plenty of bandwidth available for scanning: X for reading from S3 or cloud, Y for reading from local SSDs (cite). 
% To fully utilize the available bandwidth, you need to scan A number of large files in parallel for cloud, B number of files in parallel for disk (cite).
% Existing systems try to minimize the number of blocks that are scanned for each query - a few large blocks.
% So smaller queries might not be able to fully utilize the bandwidth -> not the best latency.
% in this paper, we introduce parallelism aware layouts that let small queries fully utilize the bandwidth -> minimizing latency.
% larger batch queries are not slower - (maybe mention: need not make the block sizes smaller)
% key idea: row group shuffling.
% say earlier in the intro that files stored in columnar format like parquet, orc, etc. they are further split into row groups and min/max values in the footer of the files for skipping.}

% \siva{the example should come in the next chapter.}
Many query workloads often come with large queries that are automated and run in batches, and interactive queries that are smaller and run one at a time.   We want to minimize the latency of all of our queries, which means we want to ensure the bandwidth is always being utilized as much as possible.  One of the capabilities that machines have to significantly increase their bandwidth is the ability to read multiple files in parallel.  This is obviously very useful for the large queries which are run in batches, as they each have to read many blocks, and reading many blocks in parallel efficiently utilizes the bandwidth.  However, the interactive queries which are run one at a time usually read very few blocks, which means that they cannot read very much data in parallel, and cannot efficiently utilize the machine's bandwidth.  This significantly increases the latency of smaller queries.\par 
% Modern partitioning schemes do not take into account an aspect of querying that nearly every modern machine uses.  Machines can usually read multiple blocks in parallel, which allows for faster reading of large numbers of blocks.  Parallelism does not, however, allow for significantly faster reading of very small numbers of blocks, so if a query is reading a very small number of blocks, then it gets no advantage from parallelism.  This means that the approaches of most modern partitioning schemes, which aim to simply minimize the number of blocks a given query is reading, do not take full advantage of parallelism.\par 
Most recent partitioning schemes attempt to minimize the latency on the workload by minimizing the number of blocks that the workload has to access. They rely on the assumption that a block is the smallest unit of I/O, which is to say that every block is either skipped or read in its entirety.  Given this assumption, minimizing the number of blocks accessed by the workload is still the objective that will minimize query time, because it is never faster to read multiple entire blocks in parallel than to read a single block in parallel.  The only way for small interactive queries to fully utilize the bandwidth of a machine, according to this assumption, is for the block size to be smaller, which would slow down the large batch queries by increasing the number of I/Os they would have to perform.  Thus, these schemes optimize to partitions which prevent the small interactive queries from fully utilizing the bandwidth, making them much slower.\par 
% For example, Pando \cite{pando} is a very recent partitioning scheme that attempts to use correlation between query predicates to minimize the number of blocks that the workload has to access.  Pando relies on the assumption that a block is the smallest unit of I/O, which is to say that every block either skipped or read in its entirety.  Given this assumption, minimizing the number of blocks accessed by the workload is still the objective that will minimize query time, because it is never faster to read multiple entire blocks in parallel than to read a single block in parallel (assuming those blocks are the same size). \par 
This assumption, however, is false, as blocks are often further subdivided into row groups, which allows for further skipping without accruing significant additional I/O cost.\par
% Like parallelism, row group skipping is a common feature of querying that are ignored by modern partitioning schemes.  But together, they can significantly speed up queries.  
Without row groups, the only way to enable small interactive queries from benefiting from parallelism would be to decrease the block size, slowing the larger queries.  But with row groups, small queries can read small sections of large blocks, allowing them to benefit from parallelism.  A query reading multiple blocks in parallel but very few row groups from each block will be significantly faster than a query reading the same number of row groups from a single block.  This means that partitioning schemes that optimize by simply minimizing the number of blocks each query in the workload has to access fail to take advantage of parallelism.  This will be further expounded upon in an example in the next chapter.
% \subsection{Overview of Modern Partitioning Schemes}
% Perhaps the simplest partitioning scheme is an even partition on one dimension.  In such a scheme, the data is sorted on one dimension and then divided into roughly equal chunks.  This partitioning scheme works well when most queries filter on only one dimension, and the data structure can easily retrieve data based on its position in that single dimension.  \par 
% However, single-dimension partitioning schemes become a lot less useful when queries filter on multiple dimensions.  As such, variations on the one-dimensional index have been developed that can take multiple dimensions into account.  Some of these variants include multi-dimensional sorts, where elements are sorted on one dimension, and ties are broken by sorting on another dimension.  One of the most commonly used variants is z-ordering \cite{z-order}, where the bits of different dimensions are interleaved to create a new dimension, which is then sorted on.\par 
% More recent partitioning schemes are based on the assumption that the best partitioning cannot be determined by the data alone, and take in a sample of queries called a \textit{workload} to help it optimize its layout.  We call a partitioning scheme that relies on a workload a \textit{workload-aware} partitioning scheme.\par
% Examples of workload-aware partitioning schemes are Flood \cite{flood}, Tsunami \cite{tsunami}, and Qd-tree \cite{qd}.  The scheme I will spend the most time focusing on in this proposal is Qd-tree, which partitions in a format much like that of a decision tree, allowing for a large amount of versatility in what structures it can represent. Qd-tree will be discussed in more detail in the related works section of this thesis.
% \section{Problem Statement}
% \subsubsection{The Drawbacks of Modern Partitioning Schemes}
% Most modern partitioning schemes attempt to minimize some approximation of the amount of time it takes to run the workload over the partitioning.  For example, Qd-tree approximates the runtime of a query using only the number of partitions that query needs to read from.  Such approximations are often flawed, because they fail to take parallelism into account.\par 
% Pando \cite{pando} is a very recent partitioning scheme that also attempts to minimize the number of blocks a query needs to access
% The first of these aspects is \textit{row group skipping}. Every partitioning scheme pays attention to how the data is divided among partitions, but aside from simple sorting, no partitioning scheme pays attention to how data is sorted \textit{within} partitions.  Most modern partitions are stored within files that use columnar layouts, such as Parquet, ORC, or Nimble, and each of these further subdivides the data into \textit{row groups}.  When querying a columnar database file, row groups which do not match the query are skipped, and not scanned at all.  However, no modern partitioning scheme cares about how row groups are organized within a partition, which means that files can take significantly longer to read.\par 
% The second of these aspects is \textit{parallel reading}.  Most modern DBMSs read multiple files in parallel instead of reading only one file at a time. For inter-query processing, this means that multiple queries can be processed at the same time, so there are most likely very little changes that can be made to a layout to optimize it for such parallel reads.  But for intra-query processing,  organizing layouts to take advantage of parallel reading can significantly speed up the reading of individual queries. \par 
% \section{Related Work}
% \subsection{Tsunami and Flood}
% As described earlier, Flood \cite{flood} is a partitioning scheme that divides data in a grid layout.  It assigns some number of columns to all but one dimension (i.e., how many ways that dimension should be split), and designates the last dimension as a "sort dimension".  It then uses an approximation of the CDF of each dimension to divide the data along each dimension that has been given columns to form a multi-dimensional grid, creating many cells.  Within each cell, points are sorted along the sort dimension. Flood optimizes its layout by testing every one of its $d$ dimensions to be the sort dimension, then picking which one allows queries to run the fastest after using gradient descent to optimize the number of columns given to the other dimensions.\par 
% Tsunami \cite{tsunami} is a partitioning scheme that was designed as an improvement over Flood less than a year after Flood debuted.  Tsunami breaks up Flood into two parts: its index and its grid.  It replaces the index with a tree structure in order to better account for skew, and calls this a Grid Tree.\par 
% One of the main drawbacks of both Tsunami and Flood is that they are not optimized for cloud databases, where accessing a single file is very expensive.  Therefore, they tend to be more willing to subdivide data far further than is optimal for a cloud database, and are thus not what we will be using going forward.\par 
% Furthermore, neither partitioning scheme is parallel-aware, so its layouts fail to take advantage of parallelism and remain suboptimal.
% \subsection{Qd-Tree}
% As stated earlier, Qd-tree \cite{qd} is a partitioning scheme for cloud data that partitions data in the shape of a decision tree.  At each node of the tree, there is a predicate, i.e, $A > 4$, or $B\;\text{IN}\;(\text{'option1', 'option2'})$.  Each node has two children: one leading to data points that agree with the predicate, and one leading to data points that do not agree with the predicate.  The leaf nodes contain partitions of all elements which are in accordance with all of the predicates they agreed with or disagreed with leading to that leaf node.\par
% There are two formulas for generating a Qd-tree.  Both of them attempt to minimize the total number of partitions that are accessed by every query in the workload, and are constrained by some block size $n$ that every partition must have size between $n$ and $2n$.  The block size restriction comes about because this is a cloud partitioning scheme, so accessing single files can be difficult.  As such, we want to make sure that partitions are not too small (so that queries do not have to access too many files), and we want to make sure that they are all a similar size, for ease of storage.\par 
% \begin{algorithm}
% \caption{Build Qd-tree}\label{alg:qd}
% \begin{algorithmic}
% \State \textbf{input} dataset $D$, workload $W$, block size $b$ 
% \If{$|D| < 2 \times b$} \Comment{If the dataset has less than $2b$ rows, then terminate}
%     \State \textbf{terminate}
% \EndIf
% \State $d \gets sample(D)$
% \State $P \gets $ all preds from $W$ and $d$
% \State $best\_score \gets -1$
% \For{$p$ in $P$} \Comment{Run through every predicate}
%     \State $d_0 \gets$ rows of $d$ that agree with $p$
%     \State $d_1 \gets$ rows of $d$ that do not agree with $p$ 
%     \State $W_0 \gets$ queries in $W$ that overlap with $p$
%     \State $W_1 \gets$ queries in $W$ that overlap with $\neg p$
%     \If{$|d_0| / |d| < b/|D|$ or $|d_1| / |d| < b/|D|$} 
%         \State $score\gets 0$ \Comment{Preds making one side smaller than the block size score 0}
%     \Else
%         \State $score \gets |d_0|\times|W-W_0| + |d_1|\times|W-W_1|$ \Comment{Number of rows the workload skips}
%     \EndIf    
%     \If{$score > best\_score$}
%         \State $best\_score \gets score$
%         \State $best\_pred \gets p$
%     \EndIf
% \EndFor
% \State $D_0 \gets$ rows of $D$ that agree with $best\_pred$
% \State $D_1 \gets$ rows of $D$ that do not agree with $best\_pred$ 
% \State $W_0 \gets$ queries in $W$ that overlap with $best\_pred$
% \State $W_1 \gets$ queries in $W$ that overlap with $\neg best\_pred$
% \State $split(D_0, W_0, b)$
% \State $split(D_1, W_1, b)$


% \end{algorithmic}
% \end{algorithm}
% The first algorithm is a greedy algorithm that tries to optimize for this independently at each node.  It simply runs the workload intersecting a given node over every possible cut, and tests which cut causes the fewest queries to have to descend down both sides.  It repeats this process for every node until a tree is generated.\par 
% The second is a reinforcement learning algorithm which operates on the entire tree.  The action space for this algorithm is the set of all possible cuts (predicates which can legally partition the subset of the data), and the state space is the superset of the data.  At each node, the algorithm will take an action by cutting the data, generating two new states to explore later.  The reward function is the number of datapoints skipped for each query at a leaf node (a state that is too small to be further cut), and the sum of the optimal reward for its children at each internal node.  A blackbox algorithm called Proximal Policy Optimization is used to update its policy.  It runs several episodes of reinforcement learning, each one ending when no node in the queue can be split further without making too small of a partition.\par 
% The main advantage of the Qd-tree is its versatility.  It can easily imitate a simple index, or a Flood grid, but it can also have much more complicated structures due to the arbitrary nature of the tree.  This allows it to perform very well at detecting specialized patterns in data and optimizing for them.  However, this versatility can also play to its detriment.  Especially with the reinforcement learning approach, it is prone to getting stuck in suboptimal formulations.\par 
% Nevertheless, its versatility allows it to work well on a wide variety of different datasets and workloads, and its simplicity makes it an easy jumping off point, which is why my work will be building off of it.  Ideally, my work will build off of the reinforcement learning algorithm, adding an additional action of turning a partition into a set of mixed partitions instead of simply splitting it.  If there is not time for that, then my work will build off of the greedy algorithm.  In both cases, the cost model will be changed to be parallel-aware.
% \subsection{Pando}
% Pando \cite{pando} is a brand-new partitioning scheme optimize to run on cloud databases.  It is similar to Qd-trees in that it generates index trees on multiple dimensions.  Also, as they are both cloud partitioning schemes, they both face similar restrictions on block size.  However, it is different from Qd-trees in that it forms multiple trees, and its trees do not function as both an index and as a partitioning scheme.  Rather, the leaf nodes of its indices can point to multiple partition blocks, and partition blocks can consist of leaf nodes of multiple index trees.  Pando can choose where to store leaf node partitions in a manner that optimizes for the closeness of correlated partitions.  \par 
% The main advantage that Pando has over Qd-trees is that it does not read every single partition at the leaf node of its trees.  Rather, it only reads the partitions that it intersects with in every single tree, which allows it to skip much more data than a Qd-tree would.\par
% Like Qd-trees, Pando is extremely versatile and has a massive space over which to optimize on its dataset and workload.  Pando also shares similarities to the row group mixing approach, in that it subdivides data beyond the minimum block size and constructs blocks from smaller pieces.  This means that Pando would significantly benefit from row group skipping and parallel-awareness, but it does not take either of those things into account. \par 
\subsection{Why This is a Difficult Problem}
Unlike trying to minimize the number of blocks that each query in the workload accesses, optimizing for parallelism is not a simple minimization problem.  This is because three things need to be optimized at once, for every single query at once:
\begin{itemize}
    \item Each small query should read fewer blocks than the number of blocks that can be read in parallel.
    \item Each small query should read from as many blocks as possible (as long as it is fewer than the number of blocks that can be read in parallel).
    \item Each query should read as few row groups as possible in the blocks it does access.
\end{itemize}
The latter two of these problems will be optimized through a process called row group mixing, which takes already created blocks and shuffles the row groups in those blocks, so that the row groups from each original block are evenly spread out among the new blocks.  Row group mixing will be covered in greater depth in the next chapter of this paper.  In order to fulfill the first of these problems, which is ensuring that small queries read fewer blocks than can be read in parallel, an algorithm would need to be very careful about which blocks it chooses to shuffle with each other.  Much of the algorithm, which will be discussed in the third chapter of this thesis, is devoted to making those decisions.
\section{Outline}
The rest of the thesis begins with a deeper discussion of modern partitioning schemes, and then a section discussing row group mixing.  This shows an example of the row group mixing algorithm being performed, explains its potential benefits, and discusses how to generalize the principles in the example to arbitrary workloads and datasets.  This is followed by the third chapter, which discusses the algorithm in complete depth.  The algorithm is then evaluated on many features such as its optimization time, query time, and number of blocks that queries access.  The thesis ends with a discussion of future work that could be done to improve this algorithm.